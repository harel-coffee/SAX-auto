{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d805fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import make_dot\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "import networkx as nx\n",
    "import pydotplus\n",
    "import datetime\n",
    "\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "\n",
    "# process mining \n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "\n",
    "# viz\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.visualization.heuristics_net import visualizer as hn_visualizer\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "\n",
    "# misc \n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e5a9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as Xet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09b9ac",
   "metadata": {},
   "source": [
    "### our LiNGAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78645bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run prior to actual algorithm runs - this is the definition of the alg\n",
    "import graphviz\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "import igraph as ig\n",
    "from scipy.special import expit as sigmoid\n",
    "import random\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"print_causal_directions\",\n",
    "    \"print_dagc\",\n",
    "    \"make_prior_knowledge\",\n",
    "    \"remove_effect\",\n",
    "    \"make_dot\",\n",
    "    \"predict_adaptive_lasso\",\n",
    "    \"get_sink_variables\",\n",
    "    \"get_exo_variables\",\n",
    "    \"find_all_paths\",\n",
    "    \"simulate_dag\",\n",
    "    \"simulate_parameter\",\n",
    "    \"simulate_linear_sem\",\n",
    "    \"count_accuracy\",\n",
    "    \"set_random_seed\",\n",
    "]\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def simulate_linear_sem(adjacency_matrix, n_samples, sem_type, noise_scale=1.0):\n",
    "    \"\"\"Simulate samples from linear SEM with specified type of noise.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like, shape (n_features, n_features)\n",
    "        Weighted adjacency matrix of DAG, where ``n_features``\n",
    "        is the number of variables.\n",
    "    n_samples : int\n",
    "        Number of samples. n_samples=inf mimics population risk.\n",
    "    sem_type : str\n",
    "        SEM type. gauss, exp, gumbel, logistic, poisson.\n",
    "    noise_scale : float\n",
    "        scale parameter of additive noise.\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data generated from linear SEM with specified type of noise,\n",
    "        where ``n_features`` is the number of variables.\n",
    "    \"\"\"\n",
    "    def _simulate_single_equation(X, w):\n",
    "        \"\"\"Simulate samples from a single equation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features_parents)\n",
    "            Data of parents for a specified variable, where\n",
    "            n_features_parents is the number of parents.\n",
    "        w : array-like, shape (1, n_features_parents)\n",
    "            Weights of parents.\n",
    "        Returns\n",
    "        -------\n",
    "        x : array-like, shape (n_samples, 1)\n",
    "            Data for the specified variable.\n",
    "        \"\"\"\n",
    "        if sem_type == 'gauss':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'exp':\n",
    "            z = np.random.exponential(scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'gumbel':\n",
    "            z = np.random.gumbel(scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'logistic':\n",
    "            x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
    "        elif sem_type == 'poisson':\n",
    "            x = np.random.poisson(np.exp(X @ w)) * 1.0\n",
    "        elif sem_type == 'subGaussian':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            q = 0.5 + 0.3 * np.random.rand(1)  # sub-Gaussian\n",
    "            z = np.sign(z) * pow(np.abs(z), q)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'supGaussian':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            q = 1.2 + 0.8 * np.random.rand(1)  # super-Gaussian\n",
    "            z = np.sign(z) * pow(np.abs(z), q)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'nonGaussian':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            qq = -1\n",
    "            if qq == 1:\n",
    "                q = 0.5 + 0.3 * np.random.rand(1)  # sub-Gaussian\n",
    "            else:\n",
    "                q = 1.2 + 0.8 * np.random.rand(1)  # super-Gaussian\n",
    "            z = np.sign(z) * pow(np.abs(z), q)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'uniform':\n",
    "            z = np.random.uniform(0, 1, n_samples)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'gamma':\n",
    "            z = np.random.gamma(2, 2, n_samples)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'laplace':\n",
    "            z = np.random.laplace(0, scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        else:\n",
    "            raise ValueError('unknown sem type')\n",
    "        return x\n",
    "\n",
    "    n_features = adjacency_matrix.shape[0]\n",
    "    if np.isinf(n_samples):\n",
    "        if sem_type == 'gauss':\n",
    "            # make 1/n_features X'X = true cov\n",
    "            X = np.sqrt(n_features) * noise_scale * np.linalg.pinv(np.eye(n_features) - adjacency_matrix)\n",
    "            return X\n",
    "        else:\n",
    "            raise ValueError('population risk not available')\n",
    "    X = np.zeros([n_samples, n_features])\n",
    "\n",
    "    G = ig.Graph.Weighted_Adjacency(adjacency_matrix.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    assert len(ordered_vertices) == n_features\n",
    "\n",
    "    for j in ordered_vertices:\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        X[:, j] = _simulate_single_equation(X[:, parents], adjacency_matrix[parents, j])\n",
    "    return X\n",
    "\n",
    "\n",
    "def count_accuracy(W_true, W, W_und=None):\n",
    "    \"\"\"Compute recalls and precisions for W, or optionally for CPDAG = W + W_und.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W_true : array-like, shape (n_features, n_features)\n",
    "        Ground truth graph, where ``n_features`` is\n",
    "        the number of features.\n",
    "    W : array-like, shape (n_features, n_features)\n",
    "        Predicted graph.\n",
    "    W_und : array-like, shape (n_features, n_features)\n",
    "        Predicted undirected edges in CPDAG, asymmetric.\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        (true positive) / (true positive + false negative).\n",
    "    precision : float\n",
    "        (true positive) / (true positive + false positive).\n",
    "    \"\"\"\n",
    "    # convert to binary adjacency matrix\n",
    "    B_true = (W_true != 0)\n",
    "    B = (W != 0)\n",
    "    B_und = None if W_und is None else (W_und != 0)\n",
    "    # linear index of nonzeros\n",
    "    pred_und = None\n",
    "    if B_und is not None:\n",
    "        pred_und = np.flatnonzero(B_und)\n",
    "    pred = np.flatnonzero(B)\n",
    "    cond = np.flatnonzero(B_true)\n",
    "    cond_reversed = np.flatnonzero(B_true.T)\n",
    "    cond_skeleton = np.concatenate([cond, cond_reversed])\n",
    "    # true pos\n",
    "    true_pos = np.intersect1d(pred, cond, assume_unique=True)\n",
    "    if B_und is not None:\n",
    "        # treat undirected edge favorably\n",
    "        true_pos_und = np.intersect1d(pred_und, cond_skeleton, assume_unique=True)\n",
    "        true_pos = np.concatenate([true_pos, true_pos_und])\n",
    "    # false pos\n",
    "    false_pos = np.setdiff1d(pred, cond_skeleton, assume_unique=True)\n",
    "    if B_und is not None:\n",
    "        false_pos_und = np.setdiff1d(pred_und, cond_skeleton, assume_unique=True)\n",
    "        false_pos = np.concatenate([false_pos, false_pos_und])\n",
    "    # reverse\n",
    "    # extra = np.setdiff1d(pred, cond, assume_unique=True)\n",
    "    # compute ratio\n",
    "    pred_size = len(pred)\n",
    "    if B_und is not None:\n",
    "        pred_size += len(pred_und)\n",
    "    # fdr = float(len(reverse) + len(false_pos)) / max(pred_size, 1)\n",
    "    tpr = float(len(true_pos)) / max(len(cond), 1)\n",
    "    # fpr = float(len(reverse) + len(false_pos)) / max(cond_neg_size, 1)\n",
    "\n",
    "    recall = tpr\n",
    "    precision = float(len(true_pos)) / max(pred_size, 1)\n",
    "\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "def simulate_parameter(B, w_ranges=((-2.0, -0.5), (0.5, 2.0))):\n",
    "    \"\"\"Simulate SEM parameters for a DAG.\n",
    "    Parameters\n",
    "    ----------\n",
    "    B : array-like, shape (n_features, n_features)\n",
    "        Binary adjacency matrix of DAG, where ``n_features``\n",
    "        is the number of features.\n",
    "    w_ranges : tuple\n",
    "        Disjoint weight ranges.\n",
    "    Returns\n",
    "    -------\n",
    "    adjacency_matrix : array-like, shape (n_features, n_features)\n",
    "        Weighted adj matrix of DAG, where ``n_features``\n",
    "        is the number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_matrix = np.zeros(B.shape)\n",
    "    S = np.random.randint(len(w_ranges), size=B.shape)  # which range\n",
    "    for i, (low, high) in enumerate(w_ranges):\n",
    "        U = np.random.uniform(low=low, high=high, size=B.shape)\n",
    "        adjacency_matrix += B * (S == i) * U\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "def simulate_dag(n_features, n_edges, graph_type):\n",
    "    \"\"\"Simulate random DAG with some expected number of edges.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "    n_edges : int\n",
    "        Expected number of edges.\n",
    "    graph_type : str\n",
    "        ER, SF.\n",
    "    Returns\n",
    "    -------\n",
    "    B : array-like, shape (n_features, n_features)\n",
    "        binary adjacency matrix of DAG.\n",
    "    \"\"\"\n",
    "    def _random_permutation(M):\n",
    "        # np.random.permutation permutes first axis only\n",
    "        P = np.random.permutation(np.eye(M.shape[0]))\n",
    "        return P.T @ M @ P\n",
    "\n",
    "    def _random_acyclic_orientation(B_und):\n",
    "        return np.tril(_random_permutation(B_und), k=-1)\n",
    "\n",
    "    def _graph_to_adjmat(G):\n",
    "        return np.array(G.get_adjacency().data)\n",
    "\n",
    "    if graph_type == 'ER':\n",
    "        # Erdos-Renyi\n",
    "        G_und = ig.Graph.Erdos_Renyi(n=n_features, m=n_edges)\n",
    "        B_und = _graph_to_adjmat(G_und)\n",
    "        B = _random_acyclic_orientation(B_und)\n",
    "    elif graph_type == 'SF':\n",
    "        # Scale-free, Barabasi-Albert\n",
    "        G = ig.Graph.Barabasi(n=n_features, m=int(round(n_edges / n_features)), directed=True)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    elif graph_type == 'BP':\n",
    "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
    "        top = int(0.2 * n_features)\n",
    "        G = ig.Graph.Random_Bipartite(top, n_features - top, m=n_edges, directed=True, neimode=ig.OUT)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    else:\n",
    "        raise ValueError('unknown graph type')\n",
    "    B_perm = _random_permutation(B)\n",
    "    assert ig.Graph.Adjacency(B_perm.tolist()).is_dag()\n",
    "    return B_perm\n",
    "\n",
    "\n",
    "def print_causal_directions(cdc, n_sampling, labels=None):\n",
    "    \"\"\"Print causal directions of bootstrap result to stdout.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cdc : dict\n",
    "        List of causal directions sorted by count in descending order.\n",
    "        This can be set the value returned by ``BootstrapResult.get_causal_direction_counts()`` method.\n",
    "    n_sampling : int\n",
    "        Number of bootstrapping samples.\n",
    "    labels : array-like, optional (default=None)\n",
    "        List of feature lables.\n",
    "        If set labels, the output feature name will be the specified label.\n",
    "    \"\"\"\n",
    "    for i, (fr, to, co) in enumerate(zip(cdc[\"from\"], cdc[\"to\"], cdc[\"count\"])):\n",
    "        sign = \"\" if \"sign\" not in cdc else \"(b>0)\" if cdc[\"sign\"][i] > 0 else \"(b<0)\"\n",
    "        if labels:\n",
    "            print(f\"{labels[to]} <--- {labels[fr]} {sign} ({100*co/n_sampling:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"x{to} <--- x{fr} {sign} ({100*co/n_sampling:.1f}%)\")\n",
    "\n",
    "\n",
    "def print_dagc(dagc, n_sampling, labels=None):\n",
    "    \"\"\"Print DAGs of bootstrap result to stdout.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dagc : dict\n",
    "        List of directed acyclic graphs sorted by count in descending order.\n",
    "        This can be set the value returned by ``BootstrapResult.get_directed_acyclic_graph_counts()`` method.\n",
    "    n_sampling : int\n",
    "        Number of bootstrapping samples.\n",
    "    labels : array-like, optional (default=None)\n",
    "        List of feature lables.\n",
    "        If set labels, the output feature name will be the specified label.\n",
    "    \"\"\"\n",
    "    for i, (dag, co) in enumerate(zip(dagc[\"dag\"], dagc[\"count\"])):\n",
    "        print(f\"DAG[{i}]: {100*co/n_sampling:.1f}%\")\n",
    "        for j, (fr, to) in enumerate(zip(dag[\"from\"], dag[\"to\"])):\n",
    "            sign = \"\" if \"sign\" not in dag else \"(b>0)\" if dag[\"sign\"][j] > 0 else \"(b<0)\"\n",
    "            if labels:\n",
    "                print(\"\\t\" + f\"{labels[to]} <--- {labels[fr]} {sign}\")\n",
    "            else:\n",
    "                print(\"\\t\" + f\"x{to} <--- x{fr} {sign}\")\n",
    "\n",
    "\n",
    "def make_prior_knowledge(\n",
    "    n_variables,\n",
    "    exogenous_variables=None,\n",
    "    sink_variables=None,\n",
    "    paths=None,\n",
    "    no_paths=None,\n",
    "):\n",
    "    \"\"\"Make matrix of prior knowledge.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_variables : int\n",
    "        Number of variables.\n",
    "    exogenous_variables : array-like, shape (index, ...), optional (default=None)\n",
    "        List of exogenous variables(index).\n",
    "        Prior knowledge is created with the specified variables as exogenous variables.\n",
    "    sink_variables : array-like, shape (index, ...), optional (default=None)\n",
    "        List of sink variables(index).\n",
    "        Prior knowledge is created with the specified variables as sink variables.\n",
    "    paths : array-like, shape ((index, index), ...), optional (default=None)\n",
    "        List of variables(index) pairs with directed path.\n",
    "        If ``(i, j)``, prior knowledge is created that xi has a directed path to xj.\n",
    "    no_paths : array-like, shape ((index, index), ...), optional (default=None)\n",
    "        List of variables(index) pairs without directed path.\n",
    "        If ``(i, j)``, prior knowledge is created that xi does not have a directed path to xj.\n",
    "    Returns\n",
    "    -------\n",
    "    prior_knowledge : array-like, shape (n_variables, n_variables)\n",
    "        Return matrix of prior knowledge used for causal discovery.\n",
    "    \"\"\"\n",
    "    prior_knowledge = np.full((n_variables, n_variables), -1)\n",
    "    if no_paths:\n",
    "        for no_path in no_paths:\n",
    "            prior_knowledge[no_path[1], no_path[0]] = 0\n",
    "    if paths:\n",
    "        for path in paths:\n",
    "            prior_knowledge[path[1], path[0]] = 1\n",
    "    if sink_variables:\n",
    "        for var in sink_variables:\n",
    "            prior_knowledge[:, var] = 0\n",
    "    if exogenous_variables:\n",
    "        for var in exogenous_variables:\n",
    "            prior_knowledge[var, :] = 0\n",
    "    np.fill_diagonal(prior_knowledge, -1)\n",
    "    return prior_knowledge\n",
    "\n",
    "\n",
    "def get_sink_variables(adjacency_matrix):\n",
    "    \"\"\"The sink variables(index) in the adjacency matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like, shape (n_variables, n_variables)\n",
    "        Adjacency matrix, where n_variables is the number of variables.\n",
    "    Returns\n",
    "    -------\n",
    "    sink_variables : array-like\n",
    "        List of sink variables(index).\n",
    "    \"\"\"\n",
    "    am = adjacency_matrix.copy()\n",
    "    am = np.abs(am)\n",
    "    np.fill_diagonal(am, 0)\n",
    "    sink_vars = [i for i in range(am.shape[1]) if am[:, i].sum() == 0]\n",
    "    return sink_vars\n",
    "\n",
    "\n",
    "def get_exo_variables(adjacency_matrix):\n",
    "    \"\"\"The exogenous variables(index) in the adjacency matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like, shape (n_variables, n_variables)\n",
    "        Adjacency matrix, where n_variables is the number of variables.\n",
    "    Returns\n",
    "    -------\n",
    "    exogenous_variables : array-like\n",
    "        List of exogenous variables(index).\n",
    "    \"\"\"\n",
    "    am = adjacency_matrix.copy()\n",
    "    am = np.abs(am)\n",
    "    np.fill_diagonal(am, 0)\n",
    "    exo_vars = [i for i in range(am.shape[1]) if am[i, :].sum() == 0]\n",
    "    return exo_vars\n",
    "\n",
    "\n",
    "def remove_effect(X, remove_features):\n",
    "    \"\"\"Create a dataset that removes the effects of features by linear regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    remove_features : array-like\n",
    "        List of features(index) to remove effects.\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data after removing effects of ``remove_features``.\n",
    "    \"\"\"\n",
    "    X = np.copy(check_array(X))\n",
    "    features_ = [i for i in np.arange(X.shape[1]) if i not in remove_features]\n",
    "    for feature in features_:\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X[:, remove_features], X[:, feature])\n",
    "        X[:, feature] = X[:, feature] - reg.predict(X[:, remove_features])\n",
    "    return X\n",
    "\n",
    "\n",
    "def make_dot(\n",
    "    adjacency_matrix,\n",
    "    labels=None,\n",
    "    lower_limit=0.01,\n",
    "    prediction_feature_indices=None,\n",
    "    prediction_target_label=\"Y(pred)\",\n",
    "    prediction_line_color=\"red\",\n",
    "    prediction_coefs=None,\n",
    "    prediction_feature_importance=None,\n",
    "    ignore_shape=False,\n",
    "):\n",
    "    \"\"\"Directed graph source code in the DOT language with specified adjacency matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like with shape (n_features, n_features)\n",
    "        Adjacency matrix to make graph, where ``n_features`` is the number of features.\n",
    "    labels : array-like, optional (default=None)\n",
    "        Label to use for graph features.\n",
    "    lower_limit : float, optional (default=0.01)\n",
    "        Threshold for drawing direction.\n",
    "        If float, then directions with absolute values of coefficients less than ``lower_limit`` are excluded.\n",
    "    prediction_feature_indices : array-like, optional (default=None)\n",
    "        Indices to use as prediction features.\n",
    "    prediction_target_label : string, optional (default='Y(pred)'))\n",
    "        Label to use for target variable of prediction.\n",
    "    prediction_line_color : string, optional (default='red')\n",
    "        Line color to use for prediction's graph.\n",
    "    prediction_coefs : array-like, optional (default=None)\n",
    "        Coefficients to use for prediction's graph.\n",
    "    prediction_feature_importance : array-like, optional (default=None)\n",
    "        Feature importance to use for prediction's graph.\n",
    "    ignore_shape : boolean, optional (default=False)\n",
    "        Ignore checking the shape of adjaceny_matrix or not.\n",
    "    Returns\n",
    "    -------\n",
    "    graph : graphviz.Digraph\n",
    "        Directed graph source code in the DOT language.\n",
    "        If order is unknown, draw a double-headed arrow.\n",
    "    \"\"\"\n",
    "    # Check parameters\n",
    "    B = check_array(np.nan_to_num(adjacency_matrix))\n",
    "    if not ignore_shape and B.shape[0] != B.shape[1]:\n",
    "        raise ValueError(\"'adjacency_matrix' is not square matrix.\")\n",
    "    if labels is not None:\n",
    "        if B.shape[1] != len(labels):\n",
    "            raise ValueError(\n",
    "                \"Length of 'labels' does not match length of 'adjacency_matrix'\"\n",
    "            )\n",
    "    if prediction_feature_indices is not None:\n",
    "        if prediction_coefs is not None and (\n",
    "            len(prediction_feature_indices) != len(prediction_coefs)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Length of 'prediction_coefs' does not match length of 'prediction_feature_indices'\"\n",
    "            )\n",
    "        if prediction_feature_importance is not None and (\n",
    "            len(prediction_feature_indices) != len(prediction_feature_importance)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Length of 'prediction_feature_importance' does not match length of 'prediction_feature_indices'\"\n",
    "            )\n",
    "\n",
    "    d = graphviz.Digraph(engine=\"dot\")\n",
    "\n",
    "    # nodes\n",
    "    names = labels if labels else [f\"x{i}\" for i in range(len(B))]\n",
    "    for name in names:\n",
    "        d.node(name)\n",
    "\n",
    "    # edges\n",
    "    idx = np.abs(B) > lower_limit\n",
    "    dirs = np.where(idx)\n",
    "    for to, from_, coef in zip(dirs[0], dirs[1], B[idx]):\n",
    "        d.edge(names[from_], names[to], label=f\"{coef:.2f}\")\n",
    "\n",
    "    # integrate of prediction model\n",
    "    if prediction_feature_indices is not None:\n",
    "        d.node(\n",
    "            prediction_target_label,\n",
    "            color=prediction_line_color,\n",
    "            fontcolor=prediction_line_color,\n",
    "        )\n",
    "\n",
    "        if prediction_coefs is not None:\n",
    "            for from_, coef in zip(prediction_feature_indices, prediction_coefs):\n",
    "                if np.abs(coef) > lower_limit:\n",
    "                    d.edge(\n",
    "                        names[from_],\n",
    "                        prediction_target_label,\n",
    "                        label=f\"{coef:.2f}\",\n",
    "                        color=prediction_line_color,\n",
    "                        fontcolor=prediction_line_color,\n",
    "                        style=\"dashed\",\n",
    "                    )\n",
    "\n",
    "        elif prediction_feature_importance is not None:\n",
    "            for from_, imp in zip(\n",
    "                prediction_feature_indices, prediction_feature_importance\n",
    "            ):\n",
    "                d.edge(\n",
    "                    names[from_],\n",
    "                    prediction_target_label,\n",
    "                    label=f\"({imp})\",\n",
    "                    color=prediction_line_color,\n",
    "                    fontcolor=prediction_line_color,\n",
    "                    style=\"dashed\",\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            for from_ in prediction_feature_indices:\n",
    "                d.edge(\n",
    "                    names[from_],\n",
    "                    prediction_target_label,\n",
    "                    color=prediction_line_color,\n",
    "                    style=\"dashed\",\n",
    "                )\n",
    "\n",
    "    # If the value is nan, draw a double-headed arrow\n",
    "    unk_order = np.where(np.isnan(np.tril(adjacency_matrix)))\n",
    "    unk_order_set = set([val for item in unk_order for val in item])\n",
    "    with d.subgraph() as s:\n",
    "        s.attr(rank=\"same\")\n",
    "        for node in unk_order_set:\n",
    "            s.node(names[node])\n",
    "    for to, from_ in zip(unk_order[0], unk_order[1]):\n",
    "        d.edge(names[from_], names[to], dir=\"both\")\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def predict_adaptive_lasso(X, predictors, target, gamma=1.0):\n",
    "    \"\"\"Predict with Adaptive Lasso.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    predictors : array-like, shape (n_predictors)\n",
    "        Indices of predictor variable.\n",
    "    target : int\n",
    "        Index of target variable.\n",
    "    Returns\n",
    "    -------\n",
    "    coef : array-like, shape (n_features)\n",
    "        Coefficients of predictor variable.\n",
    "    \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X[:, predictors], X[:, target])\n",
    "    weight = np.power(np.abs(lr.coef_), gamma)\n",
    "    reg = LassoLarsIC(criterion=\"bic\",positive=True)\n",
    "    reg.fit(X[:, predictors] * weight, X[:, target])\n",
    "    return reg.coef_ * weight\n",
    "\n",
    "\n",
    "def find_all_paths(dag, from_index, to_index, min_causal_effect=0.0):\n",
    "    \"\"\"Find all paths from point to point in DAG.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dag : array-like, shape (n_features, n_features)\n",
    "        The adjacency matrix to fine all paths, where n_features is the number of features.\n",
    "    from_index : int\n",
    "        Index of the variable at the start of the path.\n",
    "    to_index : int\n",
    "        Index of the variable at the end of the path.\n",
    "    min_causal_effect : float, optional (default=0.0)\n",
    "        Threshold for detecting causal direction.\n",
    "        Causal directions with absolute values of causal effects less than ``min_causal_effect`` are excluded.\n",
    "    Returns\n",
    "    -------\n",
    "    paths : array-like, shape (n_paths)\n",
    "        List of found path, where n_paths is the number of paths.\n",
    "    effects : array-like, shape (n_paths)\n",
    "        List of causal effect, where n_paths is the number of paths.\n",
    "    \"\"\"\n",
    "    # Extract all edges\n",
    "    edges = np.array(np.where(np.abs(np.nan_to_num(dag)) > min_causal_effect)).T\n",
    "\n",
    "    # Aggregate edges by start point\n",
    "    to_indices = []\n",
    "    for i in range(dag.shape[0]):\n",
    "        adj_list = edges[edges[:, 1] == i][:, 0].tolist()\n",
    "        if len(adj_list) != 0:\n",
    "            to_indices.append(adj_list)\n",
    "        else:\n",
    "            to_indices.append([])\n",
    "\n",
    "    # DFS\n",
    "    paths = []\n",
    "    stack = [from_index]\n",
    "    stack_to_indice = [to_indices[from_index]]\n",
    "    while stack:\n",
    "        if len(stack) > dag.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Unable to find the path because a cyclic graph has been specified.\"\n",
    "            )\n",
    "\n",
    "        cur_index = stack[-1]\n",
    "        to_indice = stack_to_indice[-1]\n",
    "\n",
    "        if cur_index == to_index:\n",
    "            paths.append(stack.copy())\n",
    "            stack.pop()\n",
    "            stack_to_indice.pop()\n",
    "        else:\n",
    "            if len(to_indice) > 0:\n",
    "                next_index = to_indice.pop(0)\n",
    "                stack.append(next_index)\n",
    "                stack_to_indice.append(to_indices[next_index].copy())\n",
    "            else:\n",
    "                stack.pop()\n",
    "                stack_to_indice.pop()\n",
    "\n",
    "    # Calculate the causal effect for each path\n",
    "    effects = []\n",
    "    for p in paths:\n",
    "        coefs = [dag[p[i + 1], p[i]] for i in range(len(p) - 1)]\n",
    "        effects.append(np.cumprod(coefs)[-1])\n",
    "\n",
    "    return paths, effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca44e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after the previous block\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_array, resample\n",
    "\n",
    "\n",
    "class BootstrapMixin:\n",
    "    \"\"\"Mixin class for all LiNGAM algorithms that implement the method of bootstrapping.\"\"\"\n",
    "\n",
    "    def bootstrap(self, X, n_sampling):\n",
    "        \"\"\"Evaluate the statistical reliability of DAG based on the bootstrapping.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where ``n_samples`` is the number of samples\n",
    "            and ``n_features`` is the number of features.\n",
    "        n_sampling : int\n",
    "            Number of bootstrapping samples.\n",
    "        Returns\n",
    "        -------\n",
    "        result : BootstrapResult\n",
    "            Returns the result of bootstrapping.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "\n",
    "        if isinstance(n_sampling, (numbers.Integral, np.integer)):\n",
    "            if not 0 < n_sampling:\n",
    "                raise ValueError(\"n_sampling must be an integer greater than 0.\")\n",
    "        else:\n",
    "            raise ValueError(\"n_sampling must be an integer greater than 0.\")\n",
    "\n",
    "        # Bootstrapping\n",
    "        adjacency_matrices = np.zeros([n_sampling, X.shape[1], X.shape[1]])\n",
    "        total_effects = np.zeros([n_sampling, X.shape[1], X.shape[1]])\n",
    "        for i in range(n_sampling):\n",
    "            self.fit(resample(X))\n",
    "            adjacency_matrices[i] = self._adjacency_matrix\n",
    "\n",
    "            # Calculate total effects\n",
    "            for c, from_ in enumerate(self._causal_order):\n",
    "                for to in self._causal_order[c + 1 :]:\n",
    "                    total_effects[i, to, from_] = self.estimate_total_effect(\n",
    "                        X, from_, to\n",
    "                    )\n",
    "\n",
    "        return BootstrapResult(adjacency_matrices, total_effects)\n",
    "\n",
    "\n",
    "class BootstrapResult(object):\n",
    "    \"\"\"The result of bootstrapping.\"\"\"\n",
    "\n",
    "    def __init__(self, adjacency_matrices, total_effects):\n",
    "        \"\"\"Construct a BootstrapResult.\n",
    "        Parameters\n",
    "        ----------\n",
    "        adjacency_matrices : array-like, shape (n_sampling)\n",
    "            The adjacency matrix list by bootstrapping.\n",
    "        total_effects : array-like, shape (n_sampling)\n",
    "            The total effects list by bootstrapping.\n",
    "        \"\"\"\n",
    "        self._adjacency_matrices = adjacency_matrices\n",
    "        self._total_effects = total_effects\n",
    "\n",
    "    @property\n",
    "    def adjacency_matrices_(self):\n",
    "        \"\"\"The adjacency matrix list by bootstrapping.\n",
    "        Returns\n",
    "        -------\n",
    "        adjacency_matrices_ : array-like, shape (n_sampling)\n",
    "            The adjacency matrix list, where ``n_sampling`` is\n",
    "            the number of bootstrap sampling.\n",
    "        \"\"\"\n",
    "        return self._adjacency_matrices\n",
    "\n",
    "    @property\n",
    "    def total_effects_(self):\n",
    "        \"\"\"The total effect list by bootstrapping.\n",
    "        Returns\n",
    "        -------\n",
    "        total_effects_ : array-like, shape (n_sampling)\n",
    "            The total effect list, where ``n_sampling`` is\n",
    "            the number of bootstrap sampling.\n",
    "        \"\"\"\n",
    "        return self._total_effects\n",
    "\n",
    "    def get_causal_direction_counts(\n",
    "        self,\n",
    "        n_directions=None,\n",
    "        min_causal_effect=None,\n",
    "        split_by_causal_effect_sign=False,\n",
    "    ):\n",
    "        \"\"\"Get causal direction count as a result of bootstrapping.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_directions : int, optional (default=None)\n",
    "            If int, then The top ``n_directions`` items are included in the result\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects\n",
    "            less than ``min_causal_effect`` are excluded.\n",
    "        split_by_causal_effect_sign : boolean, optional (default=False)\n",
    "            If True, then causal directions are split depending on the sign of the causal effect.\n",
    "        Returns\n",
    "        -------\n",
    "        causal_direction_counts : dict\n",
    "            List of causal directions sorted by count in descending order.\n",
    "            The dictionary has the following format::\n",
    "            {'from': [n_directions], 'to': [n_directions], 'count': [n_directions]}\n",
    "            where ``n_directions`` is the number of causal directions.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if isinstance(n_directions, (numbers.Integral, np.integer)):\n",
    "            if not 0 < n_directions:\n",
    "                raise ValueError(\"n_directions must be an integer greater than 0\")\n",
    "        elif n_directions is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"n_directions must be an integer greater than 0\")\n",
    "\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Count causal directions\n",
    "        directions = []\n",
    "        for am in np.nan_to_num(self._adjacency_matrices):\n",
    "            direction = np.array(np.where(np.abs(am) > min_causal_effect))\n",
    "            if split_by_causal_effect_sign:\n",
    "                signs = (\n",
    "                    np.array([np.sign(am[i][j]) for i, j in direction.T])\n",
    "                    .astype(\"int64\")\n",
    "                    .T\n",
    "                )\n",
    "                direction = np.vstack([direction, signs])\n",
    "            directions.append(direction.T)\n",
    "        directions = np.concatenate(directions)\n",
    "\n",
    "        if len(directions) == 0:\n",
    "            cdc = {\"from\": [], \"to\": [], \"count\": []}\n",
    "            if split_by_causal_effect_sign:\n",
    "                cdc[\"sign\"] = []\n",
    "            return cdc\n",
    "\n",
    "        directions, counts = np.unique(directions, axis=0, return_counts=True)\n",
    "        sort_order = np.argsort(-counts)\n",
    "        sort_order = (\n",
    "            sort_order[:n_directions] if n_directions is not None else sort_order\n",
    "        )\n",
    "        counts = counts[sort_order]\n",
    "        directions = directions[sort_order]\n",
    "\n",
    "        cdc = {\n",
    "            \"from\": directions[:, 1].tolist(),\n",
    "            \"to\": directions[:, 0].tolist(),\n",
    "            \"count\": counts.tolist(),\n",
    "        }\n",
    "        if split_by_causal_effect_sign:\n",
    "            cdc[\"sign\"] = directions[:, 2].tolist()\n",
    "\n",
    "        return cdc\n",
    "\n",
    "    def get_directed_acyclic_graph_counts(\n",
    "        self, n_dags=None, min_causal_effect=None, split_by_causal_effect_sign=False\n",
    "    ):\n",
    "        \"\"\"Get DAGs count as a result of bootstrapping.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_dags : int, optional (default=None)\n",
    "            If int, then The top ``n_dags`` items are included in the result\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects less than\n",
    "            ``min_causal_effect`` are excluded.\n",
    "        split_by_causal_effect_sign : boolean, optional (default=False)\n",
    "            If True, then causal directions are split depending on the sign of the causal effect.\n",
    "        Returns\n",
    "        -------\n",
    "        directed_acyclic_graph_counts : dict\n",
    "            List of directed acyclic graphs sorted by count in descending order.\n",
    "            The dictionary has the following format::\n",
    "            {'dag': [n_dags], 'count': [n_dags]}.\n",
    "            where ``n_dags`` is the number of directed acyclic graphs.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if isinstance(n_dags, (numbers.Integral, np.integer)):\n",
    "            if not 0 < n_dags:\n",
    "                raise ValueError(\"n_dags must be an integer greater than 0\")\n",
    "        elif n_dags is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"n_dags must be an integer greater than 0\")\n",
    "\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Count directed acyclic graphs\n",
    "        dags = []\n",
    "        for am in np.nan_to_num(self._adjacency_matrices):\n",
    "            dag = np.abs(am) > min_causal_effect\n",
    "            if split_by_causal_effect_sign:\n",
    "                direction = np.array(np.where(dag))\n",
    "                signs = np.zeros_like(dag).astype(\"int64\")\n",
    "                for i, j in direction.T:\n",
    "                    signs[i][j] = np.sign(am[i][j]).astype(\"int64\")\n",
    "                dag = signs\n",
    "            dags.append(dag)\n",
    "\n",
    "        dags, counts = np.unique(dags, axis=0, return_counts=True)\n",
    "        sort_order = np.argsort(-counts)\n",
    "        sort_order = sort_order[:n_dags] if n_dags is not None else sort_order\n",
    "        counts = counts[sort_order]\n",
    "        dags = dags[sort_order]\n",
    "\n",
    "        if split_by_causal_effect_sign:\n",
    "            dags = [\n",
    "                {\n",
    "                    \"from\": np.where(dag)[1].tolist(),\n",
    "                    \"to\": np.where(dag)[0].tolist(),\n",
    "                    \"sign\": [dag[i][j] for i, j in np.array(np.where(dag)).T],\n",
    "                }\n",
    "                for dag in dags\n",
    "            ]\n",
    "        else:\n",
    "            dags = [\n",
    "                {\"from\": np.where(dag)[1].tolist(), \"to\": np.where(dag)[0].tolist()}\n",
    "                for dag in dags\n",
    "            ]\n",
    "\n",
    "        return {\"dag\": dags, \"count\": counts.tolist()}\n",
    "\n",
    "    def get_probabilities(self, min_causal_effect=None):\n",
    "        \"\"\"Get bootstrap probability.\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects less than\n",
    "            ``min_causal_effect`` are excluded.\n",
    "        Returns\n",
    "        -------\n",
    "        probabilities : array-like\n",
    "            List of bootstrap probability matrix.\n",
    "        \"\"\"\n",
    "        # check parameters\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        adjacency_matrices = np.nan_to_num(self._adjacency_matrices)\n",
    "        shape = adjacency_matrices[0].shape\n",
    "        bp = np.zeros(shape)\n",
    "        for B in adjacency_matrices:\n",
    "            bp += np.where(np.abs(B) > min_causal_effect, 1, 0)\n",
    "        bp = bp / len(adjacency_matrices)\n",
    "\n",
    "        if int(shape[1] / shape[0]) == 1:\n",
    "            return bp\n",
    "        else:\n",
    "            return np.hsplit(bp, int(shape[1] / shape[0]))\n",
    "\n",
    "    def get_total_causal_effects(self, min_causal_effect=None):\n",
    "        \"\"\"Get total effects list.\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects less than\n",
    "            ``min_causal_effect`` are excluded.\n",
    "        Returns\n",
    "        -------\n",
    "        total_causal_effects : dict\n",
    "            List of bootstrap total causal effect sorted by probability in descending order.\n",
    "            The dictionary has the following format::\n",
    "            {'from': [n_directions], 'to': [n_directions], 'effect': [n_directions], 'probability': [n_directions]}\n",
    "            where ``n_directions`` is the number of causal directions.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Calculate probability\n",
    "        probs = np.sum(\n",
    "            np.where(np.abs(self._total_effects) > min_causal_effect, 1, 0),\n",
    "            axis=0,\n",
    "            keepdims=True,\n",
    "        )[0]\n",
    "        probs = probs / len(self._total_effects)\n",
    "\n",
    "        # Causal directions\n",
    "        dirs = np.array(np.where(np.abs(probs) > 0))\n",
    "        probs = probs[dirs[0], dirs[1]]\n",
    "\n",
    "        # Calculate median effect without zero\n",
    "        effects = np.zeros(dirs.shape[1])\n",
    "        for i, (to, from_) in enumerate(dirs.T):\n",
    "            idx = np.where(np.abs(self._total_effects[:, to, from_]) > 0)\n",
    "            effects[i] = np.median(self._total_effects[:, to, from_][idx])\n",
    "\n",
    "        # Sort by probability\n",
    "        order = np.argsort(-probs)\n",
    "        dirs = dirs.T[order]\n",
    "        effects = effects[order]\n",
    "        probs = probs[order]\n",
    "\n",
    "        ce = {\n",
    "            \"from\": dirs[:, 1].tolist(),\n",
    "            \"to\": dirs[:, 0].tolist(),\n",
    "            \"effect\": effects.tolist(),\n",
    "            \"probability\": probs.tolist(),\n",
    "        }\n",
    "\n",
    "        return ce\n",
    "\n",
    "    def get_paths(self, from_index, to_index, min_causal_effect=None):\n",
    "        \"\"\"Get all paths from the start variable to the end variable and their bootstrap probabilities.\n",
    "        Parameters\n",
    "        ----------\n",
    "        from_index : int\n",
    "            Index of the variable at the start of the path.\n",
    "        to_index : int\n",
    "            Index of the variable at the end of the path.\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            Causal directions with absolute values of causal effects less than ``min_causal_effect`` are excluded.\n",
    "        Returns\n",
    "        -------\n",
    "        paths : dict\n",
    "            List of path and bootstrap probability.\n",
    "            The dictionary has the following format::\n",
    "            {'path': [n_paths], 'effect': [n_paths], 'probability': [n_paths]}\n",
    "            where ``n_paths`` is the number of paths.\n",
    "        \"\"\"\n",
    "        # check parameters\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Find all paths from from_index to to_index\n",
    "        paths_list = []\n",
    "        effects_list = []\n",
    "        for am in self._adjacency_matrices:\n",
    "            paths, effects = find_all_paths(am, from_index, to_index, min_causal_effect)\n",
    "            # Convert path to string to make them easier to handle.\n",
    "            paths_list.extend([\"_\".join(map(str, p)) for p in paths])\n",
    "            effects_list.extend(effects)\n",
    "\n",
    "        paths_list = np.array(paths_list)\n",
    "        effects_list = np.array(effects_list)\n",
    "\n",
    "        # Count paths\n",
    "        paths_str, counts = np.unique(paths_list, axis=0, return_counts=True)\n",
    "\n",
    "        # Sort by count\n",
    "        order = np.argsort(-counts)\n",
    "        probs = counts[order] / len(self._adjacency_matrices)\n",
    "        paths_str = paths_str[order]\n",
    "\n",
    "        # Calculate median of causal effect for each path\n",
    "        effects = [\n",
    "            np.median(effects_list[np.where(paths_list == p)]) for p in paths_str\n",
    "        ]\n",
    "\n",
    "        result = {\n",
    "            \"path\": [[int(i) for i in p.split(\"_\")] for p in paths_str],\n",
    "            \"effect\": effects,\n",
    "            \"probability\": probs.tolist(),\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc337956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after previous block\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "from statsmodels.nonparametric import bandwidths\n",
    "\n",
    "__all__ = [\"get_kernel_width\", \"get_gram_matrix\", \"hsic_teststat\", \"hsic_test_gamma\"]\n",
    "\n",
    "\n",
    "def get_kernel_width(X):\n",
    "    \"\"\"Calculate the bandwidth to median distance between points.\n",
    "    Use at most 100 points (since median is only a heuristic,\n",
    "    and 100 points is sufficient for a robust estimate).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The bandwidth parameter.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    if n_samples > 100:\n",
    "        X_med = X[:100, :]\n",
    "        n_samples = 100\n",
    "    else:\n",
    "        X_med = X\n",
    "\n",
    "    G = np.sum(X_med * X_med, 1).reshape(n_samples, 1)\n",
    "    Q = np.tile(G, (1, n_samples))\n",
    "    R = np.tile(G.T, (n_samples, 1))\n",
    "\n",
    "    dists = Q + R - 2 * np.dot(X_med, X_med.T)\n",
    "    dists = dists - np.tril(dists)\n",
    "    dists = dists.reshape(n_samples ** 2, 1)\n",
    "\n",
    "    return np.sqrt(0.5 * np.median(dists[dists > 0]))\n",
    "\n",
    "\n",
    "def _rbf_dot(X, Y, width):\n",
    "    \"\"\"Compute the inner product of radial basis functions.\"\"\"\n",
    "    n_samples_X = X.shape[0]\n",
    "    n_samples_Y = Y.shape[0]\n",
    "\n",
    "    G = np.sum(X * X, 1).reshape(n_samples_X, 1)\n",
    "    H = np.sum(Y * Y, 1).reshape(n_samples_Y, 1)\n",
    "    Q = np.tile(G, (1, n_samples_Y))\n",
    "    R = np.tile(H.T, (n_samples_X, 1))\n",
    "    H = Q + R - 2 * np.dot(X, Y.T)\n",
    "\n",
    "    return np.exp(-H / 2 / (width ** 2))\n",
    "\n",
    "\n",
    "def get_gram_matrix(X, width):\n",
    "    \"\"\"Get the centered gram matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    width : float\n",
    "        The bandwidth parameter.\n",
    "    Returns\n",
    "    -------\n",
    "    K, Kc : array\n",
    "        the centered gram matrices.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    H = np.eye(n) - 1 / n * np.ones((n, n))\n",
    "\n",
    "    K = _rbf_dot(X, X, width)\n",
    "    Kc = np.dot(np.dot(H, K), H)\n",
    "\n",
    "    return K, Kc\n",
    "\n",
    "\n",
    "def hsic_teststat(Kc, Lc, n):\n",
    "    \"\"\"get the HSIC statistic.\n",
    "    Parameters\n",
    "    ----------\n",
    "    K, Kc : array\n",
    "        the centered gram matrices.\n",
    "    n : float\n",
    "        the number of samples.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        the HSIC statistic.\n",
    "    \"\"\"\n",
    "    # test statistic m*HSICb under H1\n",
    "    return 1 / n * np.sum(np.sum(Kc.T * Lc))\n",
    "\n",
    "\n",
    "def hsic_test_gamma(X, Y, bw_method=\"mdbs\"):\n",
    "    \"\"\"get the HSIC statistic.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : array-like, shape (n_samples, n_features)\n",
    "        Training data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    bw_method : str, optional (default=``mdbs``)\n",
    "        The method used to calculate the bandwidth of the HSIC.\n",
    "        * ``mdbs`` : Median distance between samples.\n",
    "        * ``scott`` : Scott's Rule of Thumb.\n",
    "        * ``silverman`` : Silverman's Rule of Thumb.\n",
    "    Returns\n",
    "    -------\n",
    "    test_stat : float\n",
    "        the HSIC statistic.\n",
    "    p : float\n",
    "        the HSIC p-value.\n",
    "    \"\"\"\n",
    "    X = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "    Y = Y.reshape(-1, 1) if Y.ndim == 1 else Y\n",
    "\n",
    "    if bw_method == \"scott\":\n",
    "        width_x = bandwidths.bw_scott(X)\n",
    "        width_y = bandwidths.bw_scott(Y)\n",
    "    elif bw_method == \"silverman\":\n",
    "        width_x = bandwidths.bw_silverman(X)\n",
    "        width_y = bandwidths.bw_silverman(Y)\n",
    "    # Get kernel width to median distance between points\n",
    "    else:\n",
    "        width_x = get_kernel_width(X)\n",
    "        width_y = get_kernel_width(Y)\n",
    "\n",
    "    # these are slightly biased estimates of centered gram matrices\n",
    "    K, Kc = get_gram_matrix(X, width_x)\n",
    "    L, Lc = get_gram_matrix(Y, width_y)\n",
    "\n",
    "    # test statistic m*HSICb under H1\n",
    "    n = X.shape[0]\n",
    "    bone = np.ones((n, 1))\n",
    "    test_stat = hsic_teststat(Kc, Lc, n)\n",
    "\n",
    "    var = (1 / 6 * Kc * Lc) ** 2\n",
    "    # second subtracted term is bias correction\n",
    "    var = 1 / n / (n - 1) * (np.sum(np.sum(var)) - np.sum(np.diag(var)))\n",
    "    # variance under H0\n",
    "    var = 72 * (n - 4) * (n - 5) / n / (n - 1) / (n - 2) / (n - 3) * var\n",
    "\n",
    "    K = K - np.diag(np.diag(K))\n",
    "    L = L - np.diag(np.diag(L))\n",
    "    mu_X = 1 / n / (n - 1) * np.dot(bone.T, np.dot(K, bone))\n",
    "    mu_Y = 1 / n / (n - 1) * np.dot(bone.T, np.dot(L, bone))\n",
    "    # mean under H0\n",
    "    mean = 1 / n * (1 + mu_X * mu_Y - mu_X - mu_Y)\n",
    "\n",
    "    alpha = mean ** 2 / var\n",
    "    # threshold for hsicArr*m\n",
    "    beta = np.dot(var, n) / mean\n",
    "    p = 1 - gamma.cdf(test_stat, alpha, scale=beta)[0][0]\n",
    "\n",
    "    return test_stat, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94365cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after previous block\n",
    "import itertools\n",
    "import warnings\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "\n",
    "class _BaseLiNGAM(BootstrapMixin, metaclass=ABCMeta):\n",
    "    \"\"\"Base class for all LiNGAM algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, random_state=None):\n",
    "        \"\"\"Construct a _BaseLiNGAM model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int, optional (default=None)\n",
    "            random_state is the seed used by the random number generator.\n",
    "        \"\"\"\n",
    "        self._random_state = random_state\n",
    "        self._causal_order = None\n",
    "        self._adjacency_matrix = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"Subclasses should implement this method!\n",
    "        Fit the model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "\n",
    "    def estimate_total_effect(self, X, from_index, to_index):\n",
    "        \"\"\"Estimate total effect using causal model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Original data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        from_index :\n",
    "            Index of source variable to estimate total effect.\n",
    "        to_index :\n",
    "            Index of destination variable to estimate total effect.\n",
    "        Returns\n",
    "        -------\n",
    "        total_effect : float\n",
    "            Estimated total effect.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "\n",
    "        # Check from/to causal order\n",
    "        from_order = self._causal_order.index(from_index)\n",
    "        to_order = self._causal_order.index(to_index)\n",
    "        if from_order > to_order:\n",
    "            warnings.warn(\n",
    "                f\"The estimated causal effect may be incorrect because \"\n",
    "                f\"the causal order of the destination variable (to_index={to_index}) \"\n",
    "                f\"is earlier than the source variable (from_index={from_index}).\"\n",
    "            )\n",
    "\n",
    "        # from_index + parents indices\n",
    "        parents = np.where(np.abs(self._adjacency_matrix[from_index]) > 0)[0]\n",
    "        predictors = [from_index]\n",
    "        predictors.extend(parents)\n",
    "\n",
    "        # Estimate total effect\n",
    "        coefs = predict_adaptive_lasso(X, predictors, to_index)\n",
    "\n",
    "        return coefs[0]\n",
    "\n",
    "    def get_error_independence_p_values(self, X):\n",
    "        \"\"\"Calculate the p-value matrix of independence between error variables.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Original data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        independence_p_values : array-like, shape (n_features, n_features)\n",
    "            p-value matrix of independence between error variables.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        E = X - np.dot(self._adjacency_matrix, X.T).T\n",
    "        p_values = np.zeros([n_features, n_features])\n",
    "        for i, j in itertools.combinations(range(n_features), 2):\n",
    "            _, p_value = hsic_test_gamma(\n",
    "                np.reshape(E[:, i], [n_samples, 1]), np.reshape(E[:, j], [n_samples, 1])\n",
    "            )\n",
    "            p_values[i, j] = p_value\n",
    "            p_values[j, i] = p_value\n",
    "\n",
    "        return p_values\n",
    "\n",
    "    def _estimate_adjacency_matrix(self, X, prior_knowledge=None):\n",
    "        \"\"\"Estimate adjacency matrix by causal order.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        prior_knowledge : array-like, shape (n_variables, n_variables), optional (default=None)\n",
    "            Prior knowledge matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        if prior_knowledge is not None:\n",
    "            pk = prior_knowledge.copy()\n",
    "            np.fill_diagonal(pk, 0)\n",
    "\n",
    "        B = np.zeros([X.shape[1], X.shape[1]], dtype=\"float64\")\n",
    "        for i in range(1, len(self._causal_order)):\n",
    "            target = self._causal_order[i]\n",
    "            predictors = self._causal_order[:i]\n",
    "\n",
    "            # Exclude variables specified in no_path with prior knowledge\n",
    "            if prior_knowledge is not None:\n",
    "                predictors = [p for p in predictors if pk[target, p] != 0]\n",
    "\n",
    "            # target is exogenous variables if predictors are empty\n",
    "            if len(predictors) == 0:\n",
    "                continue\n",
    "\n",
    "            B[target, predictors] = predict_adaptive_lasso(X, predictors, target)\n",
    "\n",
    "        self._adjacency_matrix = B\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def causal_order_(self):\n",
    "        \"\"\"Estimated causal ordering.\n",
    "        Returns\n",
    "        -------\n",
    "        causal_order_ : array-like, shape (n_features)\n",
    "            The causal order of fitted model, where\n",
    "            n_features is the number of features.\n",
    "        \"\"\"\n",
    "        return self._causal_order\n",
    "\n",
    "    @property\n",
    "    def adjacency_matrix_(self):\n",
    "        \"\"\"Estimated adjacency matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        adjacency_matrix_ : array-like, shape (n_features, n_features)\n",
    "            The adjacency matrix B of fitted model, where\n",
    "            n_features is the number of features.\n",
    "        \"\"\"\n",
    "        return self._adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d03c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after previuos block\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "class DirectLiNGAM(_BaseLiNGAM):\n",
    "    \"\"\"Implementation of DirectLiNGAM Algorithm [1]_ [2]_\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] S. Shimizu, T. Inazumi, Y. Sogawa, A. Hyvrinen, Y. Kawahara, T. Washio, P. O. Hoyer and K. Bollen.\n",
    "       DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model.\n",
    "       Journal of Machine Learning Research, 12(Apr): 1225--1248, 2011.\n",
    "    .. [2] A. Hyvrinen and S. M. Smith. Pairwise likelihood ratios for estimation of non-Gaussian structural eauation models.\n",
    "       Journal of Machine Learning Research 14:111-152, 2013.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_state=None,\n",
    "        prior_knowledge=None,\n",
    "        apply_prior_knowledge_softly=False,\n",
    "        measure=\"pwling\",\n",
    "    ):\n",
    "        \"\"\"Construct a DirectLiNGAM model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int, optional (default=None)\n",
    "            ``random_state`` is the seed used by the random number generator.\n",
    "        prior_knowledge : array-like, shape (n_features, n_features), optional (default=None)\n",
    "            Prior knowledge used for causal discovery, where ``n_features`` is the number of features.\n",
    "            The elements of prior knowledge matrix are defined as follows [1]_:\n",
    "            * ``0`` : :math:`x_i` does not have a directed path to :math:`x_j`\n",
    "            * ``1`` : :math:`x_i` has a directed path to :math:`x_j`\n",
    "            * ``-1`` : No prior knowledge is available to know if either of the two cases above (0 or 1) is true.\n",
    "        apply_prior_knowledge_softly : boolean, optional (default=False)\n",
    "            If True, apply prior knowledge softly.\n",
    "        measure : {'pwling', 'kernel'}, optional (default='pwling')\n",
    "            Measure to evaluate independence: 'pwling' [2]_ or 'kernel' [1]_.\n",
    "        \"\"\"\n",
    "        super().__init__(random_state)\n",
    "        self._Aknw = prior_knowledge\n",
    "        self._apply_prior_knowledge_softly = apply_prior_knowledge_softly\n",
    "        self._measure = measure\n",
    "\n",
    "        if self._Aknw is not None:\n",
    "            self._Aknw = check_array(self._Aknw)\n",
    "            self._Aknw = np.where(self._Aknw < 0, np.nan, self._Aknw)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where ``n_samples`` is the number of samples\n",
    "            and ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Check prior knowledge\n",
    "        if self._Aknw is not None:\n",
    "            if (n_features, n_features) != self._Aknw.shape:\n",
    "                raise ValueError(\n",
    "                    \"The shape of prior knowledge must be (n_features, n_features)\"\n",
    "                )\n",
    "            else:\n",
    "                # Extract all partial orders in prior knowledge matrix\n",
    "                if not self._apply_prior_knowledge_softly:\n",
    "                    self._partial_orders = self._extract_partial_orders(self._Aknw)\n",
    "\n",
    "        # Causal discovery\n",
    "        U = np.arange(n_features)\n",
    "        K = []\n",
    "        X_ = np.copy(X)\n",
    "        if self._measure == \"kernel\":\n",
    "            X_ = scale(X_)\n",
    "\n",
    "        for _ in range(n_features):\n",
    "            if self._measure == \"kernel\":\n",
    "                m = self._search_causal_order_kernel(X_, U)\n",
    "            else:\n",
    "                m = self._search_causal_order(X_, U)\n",
    "            for i in U:\n",
    "                if i != m:\n",
    "                    X_[:, i] = self._residual(X_[:, i], X_[:, m])\n",
    "            K.append(m)\n",
    "            U = U[U != m]\n",
    "            # Update partial orders\n",
    "            if (self._Aknw is not None) and (not self._apply_prior_knowledge_softly):\n",
    "                self._partial_orders = self._partial_orders[\n",
    "                    self._partial_orders[:, 0] != m\n",
    "                ]\n",
    "\n",
    "        self._causal_order = K\n",
    "        return self._estimate_adjacency_matrix(X, prior_knowledge=self._Aknw)\n",
    "\n",
    "    def _extract_partial_orders(self, pk):\n",
    "        \"\"\"Extract partial orders from prior knowledge.\"\"\"\n",
    "        path_pairs = np.array(np.where(pk == 1)).transpose()\n",
    "        no_path_pairs = np.array(np.where(pk == 0)).transpose()\n",
    "\n",
    "        # Check for inconsistencies in pairs with path\n",
    "        check_pairs = np.concatenate([path_pairs, path_pairs[:, [1, 0]]])\n",
    "        if len(check_pairs) > 0:\n",
    "            pairs, counts = np.unique(check_pairs, axis=0, return_counts=True)\n",
    "            if len(pairs[counts > 1]) > 0:\n",
    "                raise ValueError(\n",
    "                    f\"The prior knowledge contains inconsistencies at the following indices: {pairs[counts>1].tolist()}\"\n",
    "                )\n",
    "\n",
    "        # Check for inconsistencies in pairs without path.\n",
    "        # If there are duplicate pairs without path, they cancel out and are not ordered.\n",
    "        check_pairs = np.concatenate([no_path_pairs, no_path_pairs[:, [1, 0]]])\n",
    "        if len(check_pairs) > 0:\n",
    "            pairs, counts = np.unique(check_pairs, axis=0, return_counts=True)\n",
    "            check_pairs = np.concatenate([no_path_pairs, pairs[counts > 1]])\n",
    "            pairs, counts = np.unique(check_pairs, axis=0, return_counts=True)\n",
    "            no_path_pairs = pairs[counts < 2]\n",
    "\n",
    "        check_pairs = np.concatenate([path_pairs, no_path_pairs[:, [1, 0]]])\n",
    "        if len(check_pairs) == 0:\n",
    "            # If no pairs are extracted from the specified prior knowledge,\n",
    "            return check_pairs\n",
    "\n",
    "        pairs = np.unique(check_pairs, axis=0)\n",
    "        return pairs[:, [1, 0]]  # [to, from] -> [from, to]\n",
    "\n",
    "    def _residual(self, xi, xj):\n",
    "        \"\"\"The residual when xi is regressed on xj.\"\"\"\n",
    "        return xi - (np.cov(xi, xj)[0, 1] / np.var(xj)) * xj\n",
    "\n",
    "    def _entropy(self, u):\n",
    "        \"\"\"Calculate entropy using the maximum entropy approximations.\"\"\"\n",
    "        k1 = 79.047\n",
    "        k2 = 7.4129\n",
    "        gamma = 0.37457\n",
    "        return (1 + np.log(2 * np.pi)) / 2 - k1 * (\n",
    "            np.mean(np.log(np.cosh(u))) - gamma) ** 2 - k2 * (np.mean(u * np.exp((-(u ** 2)) / 2))) ** 2\n",
    "\n",
    "    def _diff_mutual_info(self, xi_std, xj_std, ri_j, rj_i):\n",
    "        \"\"\"Calculate the difference of the mutual informations.\"\"\"\n",
    "        return (self._entropy(xj_std) + self._entropy(ri_j / np.std(ri_j))) - (\n",
    "            self._entropy(xi_std) + self._entropy(rj_i / np.std(rj_i))\n",
    "        )\n",
    "\n",
    "    def _search_candidate(self, U):\n",
    "        \"\"\"Search for candidate features\"\"\"\n",
    "        # If no prior knowledge is specified, nothing to do.\n",
    "        if self._Aknw is None:\n",
    "            return U, []\n",
    "\n",
    "        # Apply prior knowledge in a strong way\n",
    "        if not self._apply_prior_knowledge_softly:\n",
    "            if len(self._partial_orders) != 0:\n",
    "                Uc = [i for i in U if i not in self._partial_orders[:, 1]]\n",
    "                return Uc, []\n",
    "            else:\n",
    "                return U, []\n",
    "\n",
    "        # Find exogenous features\n",
    "        Uc = []\n",
    "        for j in U:\n",
    "            index = U[U != j]\n",
    "            if self._Aknw[j][index].sum() == 0:\n",
    "                Uc.append(j)\n",
    "\n",
    "        # Find endogenous features, and then find candidate features\n",
    "        if len(Uc) == 0:\n",
    "            U_end = []\n",
    "            for j in U:\n",
    "                index = U[U != j]\n",
    "                if np.nansum(self._Aknw[j][index]) > 0:\n",
    "                    U_end.append(j)\n",
    "\n",
    "            # Find sink features (original)\n",
    "            for i in U:\n",
    "                index = U[U != i]\n",
    "                if self._Aknw[index, i].sum() == 0:\n",
    "                    U_end.append(i)\n",
    "            Uc = [i for i in U if i not in set(U_end)]\n",
    "\n",
    "        # make V^(j)\n",
    "        Vj = []\n",
    "        for i in U:\n",
    "            if i in Uc:\n",
    "                continue\n",
    "            if self._Aknw[i][Uc].sum() == 0:\n",
    "                Vj.append(i)\n",
    "        return Uc, Vj\n",
    "\n",
    "    def _search_causal_order(self, X, U):\n",
    "        \"\"\"Search the causal ordering.\"\"\"\n",
    "        Uc, Vj = self._search_candidate(U)\n",
    "        if len(Uc) == 1:\n",
    "            return Uc[0]\n",
    "\n",
    "        M_list = []\n",
    "        for i in Uc:\n",
    "            M = 0\n",
    "            for j in U:\n",
    "                if i != j:\n",
    "                    xi_std = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])\n",
    "                    xj_std = (X[:, j] - np.mean(X[:, j])) / np.std(X[:, j])\n",
    "                    ri_j = (\n",
    "                        xi_std\n",
    "                        if i in Vj and j in Uc\n",
    "                        else self._residual(xi_std, xj_std)\n",
    "                    )\n",
    "                    rj_i = (\n",
    "                        xj_std\n",
    "                        if j in Vj and i in Uc\n",
    "                        else self._residual(xj_std, xi_std)\n",
    "                    )\n",
    "                    M += np.min([0, self._diff_mutual_info(xi_std, xj_std, ri_j, rj_i)]) ** 2\n",
    "            M_list.append(-1.0 * M)\n",
    "        return Uc[np.argmax(M_list)]\n",
    "\n",
    "    def _mutual_information(self, x1, x2, param):\n",
    "        \"\"\"Calculate the mutual informations.\"\"\"\n",
    "        kappa, sigma = param\n",
    "        n = len(x1)\n",
    "        X1 = np.tile(x1, (n, 1))\n",
    "        K1 = np.exp(-1 / (2 * sigma ** 2) * (X1 ** 2 + X1.T ** 2 - 2 * X1 * X1.T))\n",
    "        X2 = np.tile(x2, (n, 1))\n",
    "        K2 = np.exp(-1 / (2 * sigma ** 2) * (X2 ** 2 + X2.T ** 2 - 2 * X2 * X2.T))\n",
    "\n",
    "        tmp1 = K1 + n * kappa * np.identity(n) / 2\n",
    "        tmp2 = K2 + n * kappa * np.identity(n) / 2\n",
    "        K_kappa = np.r_[np.c_[tmp1 @ tmp1, K1 @ K2], np.c_[K2 @ K1, tmp2 @ tmp2]]\n",
    "        D_kappa = np.r_[\n",
    "            np.c_[tmp1 @ tmp1, np.zeros([n, n])], np.c_[np.zeros([n, n]), tmp2 @ tmp2]\n",
    "        ]\n",
    "\n",
    "        sigma_K = np.linalg.svd(K_kappa, compute_uv=False)\n",
    "        sigma_D = np.linalg.svd(D_kappa, compute_uv=False)\n",
    "\n",
    "        return (-1 / 2) * (np.sum(np.log(sigma_K)) - np.sum(np.log(sigma_D)))\n",
    "\n",
    "    def _search_causal_order_kernel(self, X, U):\n",
    "        \"\"\"Search the causal ordering by kernel method.\"\"\"\n",
    "        Uc, Vj = self._search_candidate(U)\n",
    "        if len(Uc) == 1:\n",
    "            return Uc[0]\n",
    "\n",
    "        if X.shape[0] > 1000:\n",
    "            param = [2e-3, 0.5]\n",
    "        else:\n",
    "            param = [2e-2, 1.0]\n",
    "\n",
    "        Tkernels = []\n",
    "        for j in Uc:\n",
    "            Tkernel = 0\n",
    "            for i in U:\n",
    "                if i != j:\n",
    "                    ri_j = (\n",
    "                        X[:, i]\n",
    "                        if j in Vj and i in Uc\n",
    "                        else self._residual(X[:, i], X[:, j])\n",
    "                    )\n",
    "                    Tkernel += self._mutual_information(X[:, j], ri_j, param)\n",
    "            Tkernels.append(Tkernel)\n",
    "\n",
    "        return Uc[np.argmin(Tkernels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba6dc5",
   "metadata": {},
   "source": [
    "## logs transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1d585",
   "metadata": {},
   "source": [
    "### Parsing the xml and converting to DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "670bc122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Source</th>\n",
       "      <th>Type</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2182</td>\n",
       "      <td>EVENT 1 START</td>\n",
       "      <td>assign</td>\n",
       "      <td>2023-09-06 15:01:24.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2182</td>\n",
       "      <td>EVENT 1 START</td>\n",
       "      <td>start</td>\n",
       "      <td>2023-09-06 15:01:24.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2182</td>\n",
       "      <td>EVENT 1 START</td>\n",
       "      <td>complete</td>\n",
       "      <td>2023-09-06 15:01:24.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2182</td>\n",
       "      <td>Archive</td>\n",
       "      <td>assign</td>\n",
       "      <td>2023-09-06 15:01:24.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2182</td>\n",
       "      <td>Archive</td>\n",
       "      <td>start</td>\n",
       "      <td>2023-09-06 15:01:24.741000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2182</td>\n",
       "      <td>Archive</td>\n",
       "      <td>complete</td>\n",
       "      <td>2023-09-06 15:01:34.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2182</td>\n",
       "      <td>Paper disposal</td>\n",
       "      <td>assign</td>\n",
       "      <td>2023-09-06 15:01:34.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2182</td>\n",
       "      <td>Paper disposal</td>\n",
       "      <td>start</td>\n",
       "      <td>2023-09-06 15:01:34.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2182</td>\n",
       "      <td>Paper disposal</td>\n",
       "      <td>complete</td>\n",
       "      <td>2023-09-06 15:01:40.759362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2182</td>\n",
       "      <td>Close Application</td>\n",
       "      <td>assign</td>\n",
       "      <td>2023-09-06 15:01:40.759362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2182</td>\n",
       "      <td>Close Application</td>\n",
       "      <td>start</td>\n",
       "      <td>2023-09-06 15:01:40.759362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2182</td>\n",
       "      <td>Close Application</td>\n",
       "      <td>complete</td>\n",
       "      <td>2023-09-06 15:01:53.521483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2182</td>\n",
       "      <td>EVENT 5 END</td>\n",
       "      <td>assign</td>\n",
       "      <td>2023-09-06 15:01:53.521483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2182</td>\n",
       "      <td>EVENT 5 END</td>\n",
       "      <td>start</td>\n",
       "      <td>2023-09-06 15:01:53.521483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2182</td>\n",
       "      <td>EVENT 5 END</td>\n",
       "      <td>complete</td>\n",
       "      <td>2023-09-06 15:01:53.521483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id             Source      Type                   Timestamp\n",
       "0   2182      EVENT 1 START    assign  2023-09-06 15:01:24.741000\n",
       "1   2182      EVENT 1 START     start  2023-09-06 15:01:24.741000\n",
       "2   2182      EVENT 1 START  complete  2023-09-06 15:01:24.741000\n",
       "3   2182            Archive    assign  2023-09-06 15:01:24.741000\n",
       "4   2182            Archive     start  2023-09-06 15:01:24.741000\n",
       "5   2182            Archive  complete  2023-09-06 15:01:34.677000\n",
       "6   2182     Paper disposal    assign  2023-09-06 15:01:34.677000\n",
       "7   2182     Paper disposal     start  2023-09-06 15:01:34.677000\n",
       "8   2182     Paper disposal  complete  2023-09-06 15:01:40.759362\n",
       "9   2182  Close Application    assign  2023-09-06 15:01:40.759362\n",
       "10  2182  Close Application     start  2023-09-06 15:01:40.759362\n",
       "11  2182  Close Application  complete  2023-09-06 15:01:53.521483\n",
       "12  2182        EVENT 5 END    assign  2023-09-06 15:01:53.521483\n",
       "13  2182        EVENT 5 END     start  2023-09-06 15:01:53.521483\n",
       "14  2182        EVENT 5 END  complete  2023-09-06 15:01:53.521483"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Update the path to the event log file in CSV format\n",
    "df = pd.read_csv('C:\\Data\\Automation\\SAX\\Data\\mediator experimentation\\simulated_run_mediator.csv')\n",
    "df[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f3b7c",
   "metadata": {},
   "source": [
    "### Timestamps calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1b170f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain anchoring of timestamps to start time\n",
    "# For each process id transforms date-time values to timestamps and calculates the difference in secs from trace start time\n",
    "def flat_sources(x):\n",
    "    new_x = pd.DataFrame()   \n",
    "    \n",
    "    values = []   \n",
    "    \n",
    "    for index, row in x.iterrows():\n",
    "        values.append((pd.to_datetime(row['Timestamp']).value))\n",
    "        num = int(row['Id'])\n",
    "\n",
    "    x['Timestamp'] = pd.to_datetime(x['Timestamp'])\n",
    "    x['second'] = pd.to_timedelta(x['Timestamp'].dt.time.astype(str)).dt.total_seconds()\n",
    "\n",
    "    first_row = True\n",
    "    new_x['Process'] = str(num)    \n",
    "\n",
    "    for index, row in x.iterrows():\n",
    "        new_x['Process'] = str(num)\n",
    "        new_x[row['Source']] = [((pd.to_datetime(row['Timestamp']).value) - min(values))/10**6]\n",
    "            \n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d19e1af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\420190756\\AppData\\Local\\Temp\\ipykernel_26228\\917575355.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  new_df = df.groupby('Id')['Id','Source','Type','Timestamp'].apply(lambda x: flat_sources(x))\n"
     ]
    }
   ],
   "source": [
    "# applies the previous function for each process id\n",
    "new_df = df.groupby('Id')['Id','Source','Type','Timestamp'].apply(lambda x: flat_sources(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5fde1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>EVENT 1 START</th>\n",
       "      <th>Archive</th>\n",
       "      <th>Paper disposal</th>\n",
       "      <th>Close Application</th>\n",
       "      <th>EVENT 5 END</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9938.0</td>\n",
       "      <td>15165.594</td>\n",
       "      <td>27550.031</td>\n",
       "      <td>27550.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10626.0</td>\n",
       "      <td>16677.289</td>\n",
       "      <td>30952.722</td>\n",
       "      <td>30952.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9982.0</td>\n",
       "      <td>14997.792</td>\n",
       "      <td>28244.965</td>\n",
       "      <td>28244.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9094.0</td>\n",
       "      <td>15893.432</td>\n",
       "      <td>27978.915</td>\n",
       "      <td>27978.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7445.0</td>\n",
       "      <td>12777.254</td>\n",
       "      <td>23594.547</td>\n",
       "      <td>23594.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9790.0</td>\n",
       "      <td>14924.588</td>\n",
       "      <td>26717.660</td>\n",
       "      <td>26717.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7171.0</td>\n",
       "      <td>14073.309</td>\n",
       "      <td>24771.192</td>\n",
       "      <td>24771.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10951.0</td>\n",
       "      <td>16300.783</td>\n",
       "      <td>30683.017</td>\n",
       "      <td>30683.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8114.0</td>\n",
       "      <td>13814.590</td>\n",
       "      <td>24325.561</td>\n",
       "      <td>24325.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10755.0</td>\n",
       "      <td>16768.704</td>\n",
       "      <td>29908.914</td>\n",
       "      <td>29908.914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Process  EVENT 1 START  Archive  Paper disposal  Close Application  \\\n",
       "0          0            0.0   9938.0       15165.594          27550.031   \n",
       "1          1            0.0  10626.0       16677.289          30952.722   \n",
       "2          2            0.0   9982.0       14997.792          28244.965   \n",
       "3          3            0.0   9094.0       15893.432          27978.915   \n",
       "4          4            0.0   7445.0       12777.254          23594.547   \n",
       "...      ...            ...      ...             ...                ...   \n",
       "9994    9994            0.0   9790.0       14924.588          26717.660   \n",
       "9995    9995            0.0   7171.0       14073.309          24771.192   \n",
       "9996    9996            0.0  10951.0       16300.783          30683.017   \n",
       "9997    9997            0.0   8114.0       13814.590          24325.561   \n",
       "9998    9998            0.0  10755.0       16768.704          29908.914   \n",
       "\n",
       "      EVENT 5 END  \n",
       "0       27550.031  \n",
       "1       30952.722  \n",
       "2       28244.965  \n",
       "3       27978.915  \n",
       "4       23594.547  \n",
       "...           ...  \n",
       "9994    26717.660  \n",
       "9995    24771.192  \n",
       "9996    30683.017  \n",
       "9997    24325.561  \n",
       "9998    29908.914  \n",
       "\n",
       "[9999 rows x 6 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.reset_index(drop=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c25fec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_df['Process']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d43b9",
   "metadata": {},
   "source": [
    "### Causal discovery with Lingam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f8288d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\420190756\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lingam\\direct_lingam.py:220: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xi_std = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])\n",
      "C:\\Users\\420190756\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lingam\\direct_lingam.py:221: RuntimeWarning: invalid value encountered in true_divide\n",
      "  xj_std = (X[:, j] - np.mean(X[:, j])) / np.std(X[:, j])\n",
      "C:\\Users\\420190756\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lingam\\direct_lingam.py:148: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return xi - (np.cov(xi, xj)[0, 1] / np.var(xj)) * xj\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lingam.direct_lingam.DirectLiNGAM at 0x1d947939970>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lingam.DirectLiNGAM()\n",
    "model.fit(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "542a3cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.0.6 (20230106.0513)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"284pt\" height=\"305pt\"\n",
       " viewBox=\"0.00 0.00 284.34 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-301 280.34,-301 280.34,4 -4,4\"/>\n",
       "<!-- EVENT 1 START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>EVENT 1 START</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"79.29\" cy=\"-279\" rx=\"79.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.29\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">EVENT 1 START</text>\n",
       "</g>\n",
       "<!-- Archive -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Archive</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"215.29\" cy=\"-279\" rx=\"38.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.29\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Archive</text>\n",
       "</g>\n",
       "<!-- Paper disposal -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Paper disposal</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"150.29\" cy=\"-192\" rx=\"64.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"150.29\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Paper disposal</text>\n",
       "</g>\n",
       "<!-- Archive&#45;&gt;Paper disposal -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Archive&#45;&gt;Paper disposal</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M202.76,-261.61C193.53,-249.54 180.79,-232.88 170.17,-219\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173.01,-216.94 164.15,-211.12 167.45,-221.19 173.01,-216.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"198.79\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.00</text>\n",
       "</g>\n",
       "<!-- Close Application -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Close Application</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200.29\" cy=\"-105\" rx=\"76.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.29\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Close Application</text>\n",
       "</g>\n",
       "<!-- Archive&#45;&gt;Close Application -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Archive&#45;&gt;Close Application</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M219.16,-260.59C223.18,-239.85 228.26,-204.2 223.29,-174 221.05,-160.35 216.42,-145.75 211.91,-133.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.3,-132.67 208.41,-124.62 208.78,-135.21 215.3,-132.67\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.79\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">0.98</text>\n",
       "</g>\n",
       "<!-- Paper disposal&#45;&gt;Close Application -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Paper disposal&#45;&gt;Close Application</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M160.41,-173.8C167.32,-162.05 176.62,-146.24 184.5,-132.84\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.37,-134.87 189.42,-124.48 181.34,-131.32 187.37,-134.87\"/>\n",
       "<text text-anchor=\"middle\" x=\"189.79\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.01</text>\n",
       "</g>\n",
       "<!-- EVENT 5 END -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>EVENT 5 END</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200.29\" cy=\"-18\" rx=\"67.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.29\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">EVENT 5 END</text>\n",
       "</g>\n",
       "<!-- Close Application&#45;&gt;EVENT 5 END -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Close Application&#45;&gt;EVENT 5 END</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200.29,-86.8C200.29,-75.58 200.29,-60.67 200.29,-47.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.79,-47.98 200.29,-37.98 196.79,-47.98 203.79,-47.98\"/>\n",
       "<text text-anchor=\"middle\" x=\"212.79\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1.00</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1d9479390d0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the grapth from results of the alg run\n",
    "import os\n",
    "#On windows - update the path to Graphviz executables\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz/bin/'\n",
    "make_dot(model.adjacency_matrix_, labels = list(new_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b86693",
   "metadata": {},
   "source": [
    "### alpha on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cfdfa5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta \n",
    "df['Timestamp']= pd.to_datetime(df['Timestamp'])\n",
    "df.loc[df[\"Source\"] == \"End Event\", \"Timestamp\"] = df.loc[df[\"Source\"] == \"End Event\", \"Timestamp\"] + timedelta(seconds=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bfa23190",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventlog = df.copy()\n",
    "eventlog = eventlog[eventlog['Type']=='complete']\n",
    "del eventlog['Type']\n",
    "\n",
    "eventlog.rename(columns={'Timestamp': 'time:timestamp', 'Id': 'case:concept:name', 'Source': 'concept:name'}, inplace=True)\n",
    "\n",
    "## Convert to log format\n",
    "log = log_converter.apply(eventlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf3a2004",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # alpha miner\n",
    "net, initial_marking, final_marking = alpha_miner.apply(log)\n",
    "\n",
    "# Visualise\n",
    "gviz = pn_visualizer.apply(net, initial_marking, final_marking)\n",
    "pn_visualizer.view(gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8dfc5",
   "metadata": {},
   "source": [
    "### heuristic miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5cc8d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# heuristics miner\n",
    "heu_net = heuristics_miner.apply_heu(log)\n",
    "\n",
    "# viz\n",
    "gviz = hn_visualizer.apply(heu_net)\n",
    "hn_visualizer.view(gviz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
